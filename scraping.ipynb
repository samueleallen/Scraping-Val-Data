{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgY4irrKxvabPxkF/mxq3y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samueleallen/Scraping-Val-Data/blob/main/scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkTzsvh7Kx77",
        "outputId": "7ce2630e-76a8-46d7-fe8e-b92ae943439f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-c0b60cbe8fe7>:32: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  stats = pd.read_html(str(table))[0]\n",
            "<ipython-input-2-c0b60cbe8fe7>:60: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  player_stats = pd.read_html(str(table))[0]\n",
            "<ipython-input-2-c0b60cbe8fe7>:82: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  player_stats = pd.read_html(str(table))[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0      Use  RND  Rating2.0    ACS   K:D    ADR KAST   KPR   APR  \\\n",
            "0         NaN  (2) 40%   47       1.13  188.0  1.10  126.9  70%  0.68  0.30   \n",
            "1         NaN  (2) 40%   40       1.43  250.0  1.61  146.7  83%  0.93  0.38   \n",
            "2         NaN  (1) 20%   20       0.50  154.0  0.53  105.7  60%  0.50  0.15   \n",
            "\n",
            "   FKPR  FDPR   K   D   A  FK  FD  Agent  \n",
            "0  0.06  0.02  32  29  14   3   1  astra  \n",
            "1  0.13  0.10  37  23  15   5   4   omen  \n",
            "2  0.05  0.20  10  19   3   1   4  viper  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-c0b60cbe8fe7>:137: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  all_map_stats = pd.read_html(str(table))[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.vlr.gg/429395/gentle-mates-vs-fnatic-champions-tour-2025-emea-kickoff-lr2/?map=1&game=196117&tab=overview\n",
            "https://www.vlr.gg/429395/gentle-mates-vs-fnatic-champions-tour-2025-emea-kickoff-lr2/?map=2&game=196118&tab=overview\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: scrape team urls from standings page\n",
        "standings_url = 'https://www.vlr.gg/vct-2024/standings'\n",
        "data = requests.get(standings_url)\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(data.text)\n",
        "\n",
        "# Locate team urls\n",
        "standings_table = soup.select('div.eg-standing-container')[0]\n",
        "links = standings_table.find_all('a')\n",
        "links = [l.get('href') for l in links]\n",
        "links = [l for l in links if '/team/' in l]\n",
        "# Format link\n",
        "team_urls = [f'https://www.vlr.gg{l}' for l in links]\n",
        "team_urls = team_urls[0]\n",
        "data = requests.get(team_urls)\n",
        "\n",
        "# Step 2: Scrape stats section data\n",
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if l and 'team/stats' in l]\n",
        "# Successful, links = ['/team/stats/2359/leviat-n/']\n",
        "data = requests.get(f\"https://www.vlr.gg{links[0]}\")\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find('table', class_='wf-table mod-team-maps')\n",
        "\n",
        "# Use pandas to read the table\n",
        "stats = pd.read_html(str(table))[0]\n",
        "\n",
        "# Step 5: Filter rows that start with specific map names, kind of ruins table\n",
        "map_names = [\"Sunset\", \"Bind\", \"Haven\", \"Split\", \"Ascent\", \"Icebox\", \"Breeze\", \"Fracture\", \"Pearl\", \"Lotus\", \"Abyss\"]\n",
        "filtered_stats = stats[stats.iloc[:, 0].str.startswith(tuple(map_names), na=False)]\n",
        "\n",
        "# Display the filtered table\n",
        "# print(filtered_stats)\n",
        "\n",
        "# Individual Player Stats\n",
        "data = requests.get(team_urls)\n",
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if l and 'player' in l]\n",
        "# Links provides players + staff\n",
        "player_urls = [f\"https://www.vlr.gg{l}\" for l in links]\n",
        "\n",
        "# Format player stat links to entire career\n",
        "player_stats = [f\"{l}/?timespan=all\" for l in player_urls]\n",
        "\n",
        "# Keep only players, not interested in coaches + staff\n",
        "player_stats = player_stats[:5]\n",
        "data = requests.get(player_stats[0])\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find('table', class_='wf-table')\n",
        "\n",
        "# Use pandas to read the table\n",
        "player_stats = pd.read_html(str(table))[0]\n",
        "# Table has an error in first column where it can't read agent names since they are images on website\n",
        "agent_names = []\n",
        "for img_tag in table.find_all('img'):\n",
        "    img_src = img_tag.get('src') # get image sources / agent names\n",
        "    agent_name = os.path.splitext(os.path.basename(img_src))[0]\n",
        "    agent_names.append(agent_name)\n",
        "\n",
        "player_stats['Agent'] = agent_names  # Add a new 'Agent' column\n",
        "# print(player_stats)\n",
        "# now we have a new agent column that is fully functioning! 😻\n",
        "\n",
        "# Repeat player stats scraper but for recent 90 days to understand recent performances\n",
        "player_stats = [f\"{l}/?timespan=90d\" for l in player_urls]\n",
        "\n",
        "# Keep only players, not interested in coaches + staff 🐈\n",
        "player_stats = player_stats[:5]\n",
        "data = requests.get(player_stats[0])\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find('table', class_='wf-table')\n",
        "\n",
        "# Use pandas to read the table\n",
        "player_stats = pd.read_html(str(table))[0]\n",
        "# Table has an error in first column where it can't read agent names since they are images on website 😿\n",
        "agent_names = []\n",
        "for img_tag in table.find_all('img'):\n",
        "    img_src = img_tag.get('src') # get image sources / agent names\n",
        "    agent_name = os.path.splitext(os.path.basename(img_src))[0]\n",
        "    agent_names.append(agent_name)\n",
        "\n",
        "player_stats['Agent'] = agent_names  # Add a new 'Agent' column\n",
        "\n",
        "# I have all-time player stats, past 90 days player stats, team map stats, now we need match up stats for teams 😸\n",
        "# Ex: fnatic vs sentinels, sentinels has won x out of y matchups\n",
        "matches_url = 'https://www.vlr.gg/matches'\n",
        "data = requests.get(matches_url)\n",
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if l and 'page' in l]\n",
        "url = \"https://www.vlr.gg/matches/results\"  # Replace with the actual URL\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all page number links\n",
        "page_links = soup.find_all(\"a\", class_=\"btn mod-page\")\n",
        "# Page_links holds a list of HTML <a> elements beautiful soup found\n",
        "# Now extract the page numbers\n",
        "page_numbers = [int(link.text) for link in page_links if link.text.isdigit()]\n",
        "outer_list = []\n",
        "\n",
        "# Get the max page number\n",
        "max_page = max(page_numbers)\n",
        "for page in range(1, 3): # For loop starts from page 1 ends at max page\n",
        "# FIX LOOP BOUNDS LATER 🐱‍🐉🐱‍🐉🐱‍🐉🐱‍🐉🐱‍🐉\n",
        "    url = f\"https://www.vlr.gg/matches/results/?page={page}\"\n",
        "    data = requests.get(url)\n",
        "    soup = BeautifulSoup(data.text, \"html.parser\")\n",
        "    links = soup.find_all('a')\n",
        "    links = [l.get(\"href\") for l in links]\n",
        "    # Filter links to only include recent years, champions tour matches, and not acension champions tour matches\n",
        "    links = [l for l in links if l and 'champions-tour' in l and ('2023' in l or '2024' in l or '2025' in l) and 'ascension' not in l and 'challengers' not in l]\n",
        "    matches = [f\"https://www.vlr.gg{l}\" for l in links]\n",
        "    # matches is a list of lists? Erm... what the sigma? 😾\n",
        "    # note: I was wrong, i was being a tad bit silly innit 😽\n",
        "    # append each link to a different list so all elements stay in one list throughout the 550+ loops\n",
        "    outer_list.extend(matches)\n",
        "    # I DID IT!!! YIPPEE!!!! 😸😸😸\n",
        "\n",
        "# Outer list holds every vct match from 2022-current time\n",
        "# print(outer_list)\n",
        "data = requests.get(outer_list[0])\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find('table', class_='wf-table-inset mod-overview')\n",
        "\n",
        "# Use pandas to read the table\n",
        "all_map_stats = pd.read_html(str(table))[0]\n",
        "# all_map_stats.head()\n",
        "# minor error, table gets confusing. Elements of three which go from these stats categories \"all -> attack -> defend\"\n",
        "# Need to get data href for each map played\n",
        "# And need to get each data-game id\n",
        "data = requests.get(outer_list[0]) # will need a for loop later to go through every element of outer list 🐈\n",
        "\n",
        "# Locate team urls\n",
        "# maps = soup.select('vm-stats-gamesnav-item js-map-switch')[0]\n",
        "soup = BeautifulSoup(data.text, 'html.parser')\n",
        "# need amount of maps played for max index range\n",
        "divs = soup.find_all('div', class_='vm-stats-gamesnav-item js-map-switch')\n",
        "\n",
        "# Find all div elements with the given class\n",
        "for i in range(0, len(divs)):\n",
        "  divs = soup.find_all('div', class_='vm-stats-gamesnav-item js-map-switch')[i]\n",
        "  # Extract and print the 'data-href' attributes\n",
        "  data_href = divs.get('data-href')\n",
        "  game_id = divs.get(\"data-game-id\")\n",
        "  if data_href and game_id:\n",
        "    final_url = f\"https://www.vlr.gg{data_href}&game={game_id}&tab=overview\"\n",
        "    print(final_url)\n",
        "\n",
        "# Successfully fetched each match's map data. 😸\n",
        "# now clean data?\n",
        "# implement for loops\n",
        "# when cleaning data, check each table for multi leveled indexes, aka extra headers\n",
        "# if extra headers, watch 18:40 of dataquest vid\n",
        "# then lastly write to csv file? 35:00 in dataquest"
      ]
    }
  ]
}