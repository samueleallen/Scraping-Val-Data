{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1uV4xu/DB9N+IstK5lUwv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samueleallen/Scraping-Val-Data/blob/main/scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0NFjL9r9U4hP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkTzsvh7Kx77",
        "outputId": "80ffd5a4-cf38-49e1-87b1-eda6d13e435d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-20555fa8b59e>:32: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  stats = pd.read_html(str(table))[0]\n",
            "<ipython-input-13-20555fa8b59e>:60: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  player_stats = pd.read_html(str(table))[0]\n",
            "<ipython-input-13-20555fa8b59e>:82: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  player_stats = pd.read_html(str(table))[0]\n",
            "<ipython-input-13-20555fa8b59e>:132: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  all_map_stats = pd.read_html(str(table))[1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  Unnamed: 1            R2.0          ACS         K           D  \\\n",
            "0     kamo TL         NaN  1.42 1.33 1.55  290 285 297  22 12 10  / 14 8 6 /   \n",
            "1  paTiTek TL         NaN  1.28 1.41 1.10  204 196 216    15 9 6  / 11 5 6 /   \n",
            "2     nAts TL         NaN  1.27 1.18 1.39  224 224 227   18 10 8  / 14 9 5 /   \n",
            "3    kamyk TL         NaN  1.16 1.33 0.94  200 250 135   15 11 4  / 15 9 6 /   \n",
            "4    Keiko TL         NaN  0.82 0.87 0.74  162 153 174    11 6 5  / 16 9 7 /   \n",
            "\n",
            "         A       +/‚Äì          KAST          ADR          HS%     FK     FD  \\\n",
            "0    3 3 0  +8 +4 +4   67% 67% 67%  176 174 178  27% 31% 23%  6 5 1  3 3 0   \n",
            "1   11 5 6   +4 +4 0  86% 75% 100%  147 141 155  37% 33% 43%  1 0 1  3 2 1   \n",
            "2    7 4 3  +4 +1 +3   86% 83% 89%  148 156 137  41% 48% 33%  1 0 1  2 1 1   \n",
            "3    5 2 3   0 +2 -2   90% 92% 89%   146 184 95  29% 32% 24%  1 0 1  0 0 0   \n",
            "4  12 10 2  -5 -3 -2   81% 92% 67%  114 106 124  36% 40% 31%  2 0 2  2 1 1   \n",
            "\n",
            "      +/‚Äì.1  \n",
            "0  +3 +2 +1  \n",
            "1   -2 -2 0  \n",
            "2   -1 -1 0  \n",
            "3   +1 0 +1  \n",
            "4   0 -1 +1  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-20555fa8b59e>:162: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  individual_map_stats = pd.read_html(str(table))[0]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Step 1: scrape team urls from standings page\n",
        "standings_url = 'https://www.vlr.gg/vct-2024/standings'\n",
        "data = requests.get(standings_url)\n",
        "soup = BeautifulSoup(data.text)\n",
        "\n",
        "# Locate team urls\n",
        "standings_table = soup.select('div.eg-standing-container')[0]\n",
        "links = standings_table.find_all('a')\n",
        "links = [l.get('href') for l in links]\n",
        "links = [l for l in links if '/team/' in l]\n",
        "# Format link\n",
        "team_urls = [f'https://www.vlr.gg{l}' for l in links]\n",
        "team_urls = team_urls[0] # for loop later\n",
        "data = requests.get(team_urls)\n",
        "\n",
        "# Step 2: Scrape stats section data\n",
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if l and 'team/stats' in l]\n",
        "# Successful, links = ['/team/stats/2359/leviat-n/']\n",
        "data = requests.get(f\"https://www.vlr.gg{links[0]}\")\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find('table', class_='wf-table mod-team-maps')\n",
        "\n",
        "# Use pandas to read the table\n",
        "stats = pd.read_html(str(table))[0]\n",
        "\n",
        "# Step 5: Filter rows that start with specific map names, kind of ruins table\n",
        "map_names = [\"Sunset\", \"Bind\", \"Haven\", \"Split\", \"Ascent\", \"Icebox\", \"Breeze\", \"Fracture\", \"Pearl\", \"Lotus\", \"Abyss\"]\n",
        "filtered_stats = stats[stats.iloc[:, 0].str.startswith(tuple(map_names), na=False)]\n",
        "\n",
        "# Display the filtered table\n",
        "# print(filtered_stats)\n",
        "\n",
        "# Individual Player Stats\n",
        "data = requests.get(team_urls)\n",
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if l and 'player' in l]\n",
        "# Links provides players + staff\n",
        "player_urls = [f\"https://www.vlr.gg{l}\" for l in links]\n",
        "\n",
        "# Format player stat links to entire career\n",
        "player_stats = [f\"{l}/?timespan=all\" for l in player_urls]\n",
        "\n",
        "# Keep only players, not interested in coaches + staff\n",
        "player_stats = player_stats[:5]\n",
        "data = requests.get(player_stats[0])\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find('table', class_='wf-table')\n",
        "\n",
        "# Use pandas to read the table\n",
        "player_stats = pd.read_html(str(table))[0]\n",
        "# Table has an error in first column where it can't read agent names since they are images on website\n",
        "agent_names = []\n",
        "for img_tag in table.find_all('img'):\n",
        "    img_src = img_tag.get('src') # get image sources / agent names\n",
        "    agent_name = os.path.splitext(os.path.basename(img_src))[0]\n",
        "    agent_names.append(agent_name)\n",
        "\n",
        "player_stats['Agent'] = agent_names  # Add a new 'Agent' column\n",
        "# print(player_stats)\n",
        "# now we have a new agent column that is fully functioning! üòª\n",
        "\n",
        "# Repeat player stats scraper but for recent 90 days to understand recent performances\n",
        "player_stats = [f\"{l}/?timespan=90d\" for l in player_urls]\n",
        "\n",
        "# Keep only players, not interested in coaches + staff üêà\n",
        "player_stats = player_stats[:5]\n",
        "data = requests.get(player_stats[0])\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find('table', class_='wf-table')\n",
        "\n",
        "# Use pandas to read the table\n",
        "player_stats = pd.read_html(str(table))[0]\n",
        "# Table has an error in first column where it can't read agent names since they are images on website üòø\n",
        "agent_names = []\n",
        "for img_tag in table.find_all('img'):\n",
        "    img_src = img_tag.get('src') # get image sources / agent names\n",
        "    agent_name = os.path.splitext(os.path.basename(img_src))[0]\n",
        "    agent_names.append(agent_name)\n",
        "\n",
        "player_stats['Agent'] = agent_names # Add a new 'Agent' column\n",
        "\n",
        "# I have all-time player stats, past 90 days player stats, team map stats, now we need match up stats for teams üò∏\n",
        "# Ex: fnatic vs sentinels, sentinels has won x out of y matchups\n",
        "matches_url = 'https://www.vlr.gg/matches'\n",
        "data = requests.get(matches_url)\n",
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if l and 'page' in l]\n",
        "url = \"https://www.vlr.gg/matches/results\"  # Replace with the actual URL\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all page number links\n",
        "page_links = soup.find_all(\"a\", class_=\"btn mod-page\")\n",
        "# Page_links holds a list of HTML <a> elements beautiful soup found\n",
        "# Now extract the page numbers\n",
        "page_numbers = [int(link.text) for link in page_links if link.text.isdigit()]\n",
        "outer_list = []\n",
        "\n",
        "# Get the max page number\n",
        "max_page = max(page_numbers)\n",
        "for page in range(1, 3): # For loop starts from page 1 ends at max page\n",
        "# FIX LOOP BOUNDS LATER üê±‚Äçüêâüê±‚Äçüêâüê±‚Äçüêâüê±‚Äçüêâüê±‚Äçüêâ\n",
        "    url = f\"https://www.vlr.gg/matches/results/?page={page}\"\n",
        "    data = requests.get(url)\n",
        "    soup = BeautifulSoup(data.text, \"html.parser\")\n",
        "    links = soup.find_all('a')\n",
        "    links = [l.get(\"href\") for l in links]\n",
        "    # Filter links to only include recent years, champions tour matches, and not acension champions tour matches\n",
        "    links = [l for l in links if l and 'champions-tour' in l and ('2023' in l or '2024' in l or '2025' in l) and 'ascension' not in l and 'challengers' not in l]\n",
        "    matches = [f\"https://www.vlr.gg{l}\" for l in links]\n",
        "    # append each link to a different list so all elements stay in one list throughout the 550+ loops\n",
        "    outer_list.extend(matches)\n",
        "\n",
        "# Outer list holds every vct match from 2022-current time\n",
        "data = requests.get(outer_list[0])\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find_all('table', class_='wf-table-inset mod-overview')\n",
        "\n",
        "# Use pandas to read the table\n",
        "all_map_stats = pd.read_html(str(table))[0] # max index is 1, min index is 0, need both tables\n",
        "# minor error, table gets confusing. Elements of three which go from these stats categories \"all -> attack -> defend\"\n",
        "# additionally, it only looks at 1st team, not 2nd teams stats\n",
        "# Need to get data href for each map played\n",
        "# And need to get each data-game id\n",
        "data = requests.get(outer_list[0]) # will need a for loop later to go through every element of outer list üêà\n",
        "\n",
        "# Locate team urls\n",
        "# maps = soup.select('vm-stats-gamesnav-item js-map-switch')[0]\n",
        "soup = BeautifulSoup(data.text, 'html.parser')\n",
        "# need amount of maps played for max index range\n",
        "divs = soup.find_all('div', class_='vm-stats-gamesnav-item js-map-switch')\n",
        "all_match_tabs = []\n",
        "\n",
        "# Find all div elements with the given class\n",
        "for i in range(0, len(divs)):\n",
        "  divs = soup.find_all('div', class_='vm-stats-gamesnav-item js-map-switch')[i]\n",
        "  # Extract and print the 'data-href' attributes\n",
        "  data_href = divs.get('data-href')\n",
        "  game_id = divs.get(\"data-game-id\")\n",
        "  if data_href and game_id:\n",
        "    final_url = f\"https://www.vlr.gg{data_href}&game={game_id}&tab=overview\"\n",
        "    all_match_tabs.append(final_url)\n",
        "\n",
        "data = requests.get(all_match_tabs[0])\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find_all('table', class_='wf-table-inset mod-overview')\n",
        "\n",
        "# Use pandas to read the table\n",
        "individual_map_stats = pd.read_html(str(table))[0] # again, need both index 0 and 1 here\n",
        "\n",
        "# Successfully fetched each match's map data. üò∏\n",
        "# now clean data?\n",
        "# implement for loops\n",
        "# when cleaning data, check each table for multi leveled indexes, aka extra headers\n",
        "# if extra headers, watch 18:40 of dataquest vid\n",
        "# then lastly write to csv file? 35:00 in dataquest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_requests(request_var):\n",
        "  data = requests.get(request_var)\n",
        "  soup = BeautifulSoup(data.text)"
      ],
      "metadata": {
        "id": "P997uqz_Xr--"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}