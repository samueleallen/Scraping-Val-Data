{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7lpA+p39n/zU/gx8muNt6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samueleallen/Scraping-Val-Data/blob/main/scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aag3lzCLK4nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBxjesrsc_S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "pkTzsvh7Kx77",
        "outputId": "037f7198-da1f-4dcd-8ac6-e22e91b8fe97",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e0f09dccea96>:32: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  stats = pd.read_html(str(table))[0]\n",
            "<ipython-input-3-e0f09dccea96>:60: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  player_stats = pd.read_html(str(table))[0]\n",
            "<ipython-input-3-e0f09dccea96>:82: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  player_stats = pd.read_html(str(table))[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.vlr.gg/430855/jdg-esports-vs-xi-lai-gaming-champions-tour-2025-china-kickoff-lr2\n",
            "https://www.vlr.gg/430853/xi-lai-gaming-vs-all-gamers-champions-tour-2025-china-kickoff-lr1\n",
            "https://www.vlr.gg/430840/all-gamers-vs-nova-esports-champions-tour-2025-china-kickoff-ur1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e0f09dccea96>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'champions-tour'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'2023'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'2024'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'2025'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'ascension'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'challengers'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"https://www.vlr.gg{l}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os # maybe not needed\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: scrape team urls from standings page\n",
        "standings_url = 'https://www.vlr.gg/vct-2024/standings'\n",
        "data = requests.get(standings_url)\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(data.text)\n",
        "\n",
        "# Locate team urls\n",
        "standings_table = soup.select('div.eg-standing-container')[0]\n",
        "links = standings_table.find_all('a')\n",
        "links = [l.get('href') for l in links]\n",
        "links = [l for l in links if '/team/' in l]\n",
        "# Format link\n",
        "team_urls = [f'https://www.vlr.gg{l}' for l in links]\n",
        "team_urls = team_urls[0]\n",
        "data = requests.get(team_urls)\n",
        "\n",
        "# Step 2: Scrape stats section data\n",
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if l and 'team/stats' in l]\n",
        "# Successful, links = ['/team/stats/2359/leviat-n/']\n",
        "data = requests.get(f\"https://www.vlr.gg{links[0]}\")\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find('table', class_='wf-table mod-team-maps')\n",
        "\n",
        "# Use pandas to read the table\n",
        "stats = pd.read_html(str(table))[0]\n",
        "\n",
        "# Step 5: Filter rows that start with specific map names, kind of ruins table\n",
        "map_names = [\"Sunset\", \"Bind\", \"Haven\", \"Split\", \"Ascent\", \"Icebox\", \"Breeze\", \"Fracture\", \"Pearl\", \"Lotus\", \"Abyss\"]\n",
        "filtered_stats = stats[stats.iloc[:, 0].str.startswith(tuple(map_names), na=False)]\n",
        "\n",
        "# Display the filtered table\n",
        "# print(filtered_stats)\n",
        "\n",
        "# Individual Player Stats\n",
        "data = requests.get(team_urls)\n",
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if l and 'player' in l]\n",
        "# Links provides players + staff\n",
        "player_urls = [f\"https://www.vlr.gg{l}\" for l in links]\n",
        "\n",
        "# Format player stat links to entire career\n",
        "player_stats = [f\"{l}/?timespan=all\" for l in player_urls]\n",
        "\n",
        "# Keep only players, not interested in coaches + staff\n",
        "player_stats = player_stats[:5]\n",
        "data = requests.get(player_stats[0])\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find('table', class_='wf-table')\n",
        "\n",
        "# Use pandas to read the table\n",
        "player_stats = pd.read_html(str(table))[0]\n",
        "# Table has an error in first column where it can't read agent names since they are images on website\n",
        "agent_names = []\n",
        "for img_tag in table.find_all('img'):\n",
        "    img_src = img_tag.get('src') # get image sources / agent names\n",
        "    agent_name = os.path.splitext(os.path.basename(img_src))[0]\n",
        "    agent_names.append(agent_name)\n",
        "\n",
        "player_stats['Agent'] = agent_names  # Add a new 'Agent' column\n",
        "#player_stats.head()\n",
        "# now we have a new agent column that is fully functioning!\n",
        "\n",
        "# Repeat player stats scraper but for recent 90 days to understand recent performances\n",
        "player_stats = [f\"{l}/?timespan=90d\" for l in player_urls]\n",
        "\n",
        "# Keep only players, not interested in coaches + staff\n",
        "player_stats = player_stats[:5]\n",
        "data = requests.get(player_stats[0])\n",
        "soup = BeautifulSoup(data.text)  # Create BeautifulSoup object for stats page\n",
        "table = soup.find('table', class_='wf-table')\n",
        "\n",
        "# Use pandas to read the table\n",
        "player_stats = pd.read_html(str(table))[0]\n",
        "# Table has an error in first column where it can't read agent names since they are images on website\n",
        "agent_names = []\n",
        "for img_tag in table.find_all('img'):\n",
        "    img_src = img_tag.get('src') # get image sources / agent names\n",
        "    agent_name = os.path.splitext(os.path.basename(img_src))[0]\n",
        "    agent_names.append(agent_name)\n",
        "\n",
        "player_stats['Agent'] = agent_names  # Add a new 'Agent' column\n",
        "\n",
        "# I have all-time player stats, past 90 days player stats, team map stats, now we need match up stats for teams. Ex: fnatic vs sentinels, sentinels has won x out of y matchups\n",
        "matches_url = 'https://www.vlr.gg/matches'\n",
        "data = requests.get(matches_url)\n",
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "links = [l.get(\"href\") for l in links]\n",
        "links = [l for l in links if l and 'page' in l]\n",
        "url = \"https://www.vlr.gg/matches/results\"  # Replace with the actual URL\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all page number links\n",
        "page_links = soup.find_all(\"a\", class_=\"btn mod-page\")\n",
        "# Page_links holds a list of HTML <a> elements beautiful soup found\n",
        "# Now extract the page numbers\n",
        "page_numbers = [int(link.text) for link in page_links if link.text.isdigit()]\n",
        "\n",
        "# Get the max page number\n",
        "max_page = max(page_numbers)\n",
        "\n",
        "for page in range(1, max_page+1): # For loop starts from page 1 ends at max page\n",
        "    url = f\"https://www.vlr.gg/matches/results/?page={page}\"\n",
        "    data = requests.get(url)\n",
        "    soup = BeautifulSoup(data.text, \"html.parser\")\n",
        "    links = soup.find_all('a')\n",
        "    links = [l.get(\"href\") for l in links]\n",
        "    # Filter links to only include recent years, champions tour matches, and not acension champions tour matches\n",
        "    links = [l for l in links if l and 'champions-tour' in l and ('2023' in l or '2024' in l or '2025' in l) and 'ascension' not in l and 'challengers' not in l]\n",
        "    matches = [f\"https://www.vlr.gg{l}\" for l in links]\n",
        "    print(matches[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_xthXXD-Qve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhFZtmKGdI8Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}